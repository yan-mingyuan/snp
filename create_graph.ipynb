{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import PATH_PROCESSED, DEVICE, SEED, BATCH_SIZE\n",
    "from datautils import *\n",
    "from nnutils import *\n",
    "\n",
    "# Load data\n",
    "data, gene_mapping, trait_mapping = create_graph()\n",
    "\n",
    "variant_mapping, variant_x = load_node_csv(osp.join(PATH_PROCESSED, 'variant_x.csv'), index_col='SNPs')\n",
    "# Provide labels as positive samples\n",
    "labels = pd.read_csv(osp.join(PATH_PROCESSED, 'labels.csv'), index_col='snps', converters={'hpo_id': ast.literal_eval})\n",
    "train_labels, val_labels, test_labels = split_dataset(labels, random_state=SEED)\n",
    "# Provide negative samples\n",
    "disease_to_traits = pd.read_csv(osp.join(PATH_PROCESSED, 'disease_to_traits.csv'), index_col='disease_index', converters={'hpo_id': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 1it [00:00,  1.13it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.1122201442718507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 1it [00:00,  2.20it/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.068844383955002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 1it [00:00,  2.31it/s, loss=0.977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.9769230782985687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 1it [00:00,  2.35it/s, loss=0.892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.8918221771717072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 1it [00:00,  2.10it/s, loss=0.799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.7992184281349182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 1it [00:00,  2.23it/s, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.7015470147132874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 1it [00:00,  1.99it/s, loss=0.629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6285759389400483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 1it [00:00,  2.08it/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.5672170162200928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 1it [00:00,  2.02it/s, loss=0.524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.5236829131841659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 1it [00:00,  2.30it/s, loss=0.508]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.507775217294693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "hidden_channels = 256\n",
    "out_channels = 128\n",
    "num_heads = 2\n",
    "num_graph_layers = 1\n",
    "temperature = 0.7\n",
    "\n",
    "model = OurModel(\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_heads=num_heads,\n",
    "    num_graph_layers=num_graph_layers,\n",
    "    data=data,\n",
    "    variant_input_dim=variant_x.size(1),\n",
    "    pooling_type='attention',\n",
    "    dropout_prob=0.3\n",
    ").to(DEVICE)\n",
    "\n",
    "# Define optimizer hyperparameters\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-2\n",
    "momentum_gradient = 0.9\n",
    "momentum_square = 0.95\n",
    "# optimizer = optim.AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr=learning_rate,\n",
    "#     weight_decay=weight_decay,\n",
    "#     betas=(momentum_gradient, momentum_square),\n",
    "# )\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    momentum = momentum_gradient,\n",
    "    weight_decay = weight_decay\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "num_test_samples = 30\n",
    "num_negative_samples = 32\n",
    "epochs = 30\n",
    "data = data.to(DEVICE)\n",
    "\n",
    "# Training loop with gradient descent and progress bar\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0  # Counter for batches\n",
    "\n",
    "    label_loader = label_generator(train_labels.iloc[:num_test_samples], BATCH_SIZE, gene_mapping, trait_mapping, variant_mapping)\n",
    "    progress_bar = tqdm(label_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "    \n",
    "    for label_batch in progress_bar:\n",
    "        optimizer.zero_grad()  # Reset the gradients before each batch\n",
    "        \n",
    "        # Process each label batch\n",
    "        for variant, (gene, disease, traits) in label_batch.iterrows():\n",
    "            variant_id = variant_mapping[variant]\n",
    "            gene_id = gene_mapping[gene]\n",
    "            trait_ids = [trait_mapping[trait] for trait in traits]\n",
    "            \n",
    "            # Get positive and negative samples\n",
    "            positive_relations = labels.loc[labels.index == variant, ['disease_index', 'hpo_id']]\n",
    "            positive_diseases = positive_relations['disease_index'].to_list()\n",
    "            positive_trait_groups = positive_relations['hpo_id'].to_list()\n",
    "            negative_trait_groups = sample_negative_trait_gropus(disease_to_traits, positive_diseases, num_negative_samples=num_negative_samples)\n",
    "            \n",
    "            # Combine positive and negative trait groups\n",
    "            num_positives = len(positive_relations)\n",
    "            trait_groups = positive_trait_groups + negative_trait_groups\n",
    "            batch_ids = torch.cat([torch.full((len(group),), i) for i, group in enumerate(trait_groups)])\n",
    "            trait_ids = torch.tensor([trait_mapping[trait] for trait_group in trait_groups for trait in trait_group])\n",
    "            \n",
    "            batch_ids = batch_ids.to(DEVICE)\n",
    "            trait_ids = trait_ids.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            gene_embedding, disease_embedding = model(\n",
    "                data=data,\n",
    "                variant_x=variant_x,\n",
    "                variant_id=variant_id,\n",
    "                gene_id=gene_id,\n",
    "                trait_ids=trait_ids,\n",
    "                batch_ids=batch_ids\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            # loss = multi_positive_info_nce_loss(gene_embedding, disease_embedding, num_positives, temperature)\n",
    "            loss = cosine_margin_loss(gene_embedding, disease_embedding, num_positives, margin=0.5)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss and update progress bar\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1  # Increment the batch counter\n",
    "            progress_bar.set_postfix({'loss': running_loss / num_batches})\n",
    "    \n",
    "    # Print loss at the end of the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
