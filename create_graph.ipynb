{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene features:       (8127, 200)\n",
      "Trait features:      (8526, 768)\n",
      "Gene-to-gene edges:   171534 edges.\n",
      "Gene-to-trait edges:  221916 edges.\n",
      "Trait-to-trait edges: 7732 edges.\n",
      "Variants features:   (10547, 768)\n",
      "Gene local features: (10547, 768)\n",
      "Variants associated with diseases and trait groups: Total = 12914 (Train: 9039, Validation: 1937, Test: 1938)\n",
      "Number of diseases: 848\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import wandb\n",
    "\n",
    "from config import PATH_PROCESSED, DEVICE, SEED, set_seed_all\n",
    "from datautils import create_graph, load_node_csv, split_dataset, LabelGenerator, sample_negative_trait_groups\n",
    "from nnutils import *\n",
    "\n",
    "set_seed_all(SEED)\n",
    "\n",
    "# Step 1: Load graph data\n",
    "# Creates the graph structure and loads node mappings\n",
    "data, gene_mapping, trait_mapping = create_graph()\n",
    "data = data.to(DEVICE)\n",
    "\n",
    "# Step 2: Load variant features\n",
    "# Reads variant data and feature matrix\n",
    "variant_mapping, variant_x = load_node_csv(\n",
    "    osp.join(PATH_PROCESSED, 'variant_x.csv'), index_col='SNPs'\n",
    ")\n",
    "_, gene_local_x = load_node_csv(\n",
    "    osp.join(PATH_PROCESSED, 'gene_local_x.csv'), index_col=0\n",
    ")\n",
    "print(f\"Variants features:   ({variant_x.size(0)}, {variant_x.size(1)})\")\n",
    "print(f\"Gene local features: ({variant_x.size(0)}, {variant_x.size(1)})\")\n",
    "\n",
    "# Step 3: Load labels and split datasets\n",
    "labels = pd.read_csv(\n",
    "    osp.join(PATH_PROCESSED, 'labels.csv'),\n",
    "    index_col='snps',\n",
    "    converters={'hpo_id': ast.literal_eval}\n",
    ")\n",
    "train_labels, val_labels, test_labels = split_dataset(\n",
    "    labels, random_state=SEED)\n",
    "print(f\"Variants associated with diseases and trait groups: \"\n",
    "      f\"Total = {labels.shape[0]} (Train: {train_labels.shape[0]}, \"\n",
    "      f\"Validation: {val_labels.shape[0]}, Test: {test_labels.shape[0]})\")\n",
    "\n",
    "# Step 4: Load disease-to-trait mapping for negative sampling\n",
    "# Read disease-to-trait mappings, converting 'hpo_id' into list format\n",
    "disease_to_traits = pd.read_csv(\n",
    "    osp.join(PATH_PROCESSED, 'disease_to_traits.csv'),\n",
    "    index_col='disease_index',\n",
    "    converters={'hpo_id': ast.literal_eval}\n",
    ")\n",
    "print(f\"Number of diseases: {disease_to_traits.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wo30283o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AUC</td><td>▁██▅</td></tr><tr><td>AUPRC</td><td>▁█▄▄</td></tr><tr><td>MRR</td><td>▁▇█▆</td></tr><tr><td>train_batch_loss</td><td>▆▆▄▃▃▄▃▄▁▃▂▃▄▁▂▂▂▃▃▁▂▂▅▁▃▄▁▂▁▅▁▅▃█▃▂▁▃▄▄</td></tr><tr><td>valid_batch_loss</td><td>██████████▄▆▅▄▆▃▃▅▆▁▄▄▄▃▆▄▃▃▂▅▆▃▄▂▅▄▆█▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AUC</td><td>0.76585</td></tr><tr><td>AUPRC</td><td>0.20054</td></tr><tr><td>MRR</td><td>0.44457</td></tr><tr><td>train_batch_loss</td><td>8.09402</td></tr><tr><td>valid_batch_loss</td><td>5.95413</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-wood-1</strong> at: <a href='https://wandb.ai/radiant_frontiers/snp/runs/wo30283o' target=\"_blank\">https://wandb.ai/radiant_frontiers/snp/runs/wo30283o</a><br/> View project at: <a href='https://wandb.ai/radiant_frontiers/snp' target=\"_blank\">https://wandb.ai/radiant_frontiers/snp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240906_193516-wo30283o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wo30283o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/amax/wjw/variants/wandb/run-20240906_200247-7vp85hb5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/radiant_frontiers/snp/runs/7vp85hb5' target=\"_blank\">eager-spaceship-2</a></strong> to <a href='https://wandb.ai/radiant_frontiers/snp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/radiant_frontiers/snp' target=\"_blank\">https://wandb.ai/radiant_frontiers/snp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/radiant_frontiers/snp/runs/7vp85hb5' target=\"_blank\">https://wandb.ai/radiant_frontiers/snp/runs/7vp85hb5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/radiant_frontiers/snp/runs/7vp85hb5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3028d5adc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_channels = 200\n",
    "out_channels = 128\n",
    "num_heads = 2\n",
    "num_graph_layers = 2\n",
    "pooling_type = 'attention'\n",
    "dropout_prob = 0.3\n",
    "temperature = 0.7\n",
    "norm = True\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "momentum_gradient = 0.9\n",
    "momentum_square = 0.95\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "# Metrics vary in different settings\n",
    "batch_size = 2\n",
    "num_train_samples = 1000\n",
    "num_valid_samples = 1000\n",
    "max_positives = None\n",
    "max_num_negatives = None\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"snp\", \n",
    "    config={\n",
    "        \"hidden_channels\": hidden_channels,\n",
    "        \"out_channels\": out_channels,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"num_graph_layers\": num_graph_layers,\n",
    "        \"pooling_type\": pooling_type,\n",
    "        \"dropout_prob\": dropout_prob,\n",
    "        \"temperature\": temperature,\n",
    "        \"norm\": norm,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"momentum_gradient\": momentum_gradient,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_epochs\": max_epochs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test() takes 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 188\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# print(f\"\\nEpoch {epoch+1}/{max_epochs}\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# wandb.log({\"epoch\": epoch + 1})\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# print(f\"\\nValidation after Epoch {epoch+1}\")\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m         \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     train(model, optimizer, train_loader,\n\u001b[1;32m    191\u001b[0m           max_num_negatives, max_positives, temperature, norm)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# print(f\"\\nFinal Validation after Epoch {max_epochs}\")\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: test() takes 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, label_loader, num_negatives, max_positives=None, temperature=0.7, norm=False):\n",
    "    \"\"\"Train the model with a given optimizer and label loader.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        label_loader, desc=f\"Epoch {epoch+1}/{max_epochs}\", leave=False, ncols=100)\n",
    "    # progress_bar = tqdm(\n",
    "        # label_loader, desc=f\"Epoch [{epoch+1}/{max_epochs}] - Training\", leave=False, ncols=100, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\")\n",
    "\n",
    "    for label_batch in progress_bar:\n",
    "        optimizer.zero_grad()  # Reset the gradients before each batch\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        # Process each label batch\n",
    "        for variant, (gene, disease, traits) in label_batch.iterrows():\n",
    "            variant_id = variant_mapping[variant]\n",
    "            gene_id = gene_mapping[gene]\n",
    "\n",
    "            # Get positive and negative samples\n",
    "            positive_relations = labels.loc[labels.index == variant, [\n",
    "                'disease_index', 'hpo_id']]\n",
    "            if max_positives is not None:\n",
    "                positive_relations = positive_relations.sample(\n",
    "                    n=min(max_positives, len(positive_relations)))\n",
    "            positive_diseases = positive_relations['disease_index'].to_list()\n",
    "            positive_trait_groups = positive_relations['hpo_id'].to_list()\n",
    "\n",
    "            # Sample negative trait groups\n",
    "            negative_trait_groups = sample_negative_trait_groups(\n",
    "                disease_to_traits, positive_diseases, num_negatives)\n",
    "\n",
    "            # Combine positive and negative trait groups\n",
    "            num_positives = len(positive_relations)\n",
    "            trait_groups = positive_trait_groups + negative_trait_groups\n",
    "            batch_ids = torch.cat([torch.full((len(group),), i)\n",
    "                                   for i, group in enumerate(trait_groups)])\n",
    "            trait_ids = torch.tensor(\n",
    "                [trait_mapping[trait] for trait_group in trait_groups for trait in trait_group])\n",
    "\n",
    "            # Move to device (GPU)\n",
    "            batch_ids = batch_ids.to(DEVICE)\n",
    "            trait_ids = trait_ids.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            gene_embedding, disease_embedding = model(\n",
    "                variant_id=variant_id,\n",
    "                gene_id=gene_id,\n",
    "                trait_ids=trait_ids,\n",
    "                batch_ids=batch_ids\n",
    "            )\n",
    "\n",
    "            # Compute loss (use InfoNCE loss)\n",
    "            loss = multi_positive_info_nce_loss(\n",
    "                gene_embedding, disease_embedding, num_positives, temperature, norm=True)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        # Accumulate loss and update progress bar\n",
    "        running_loss += batch_loss / len(label_batch)\n",
    "        num_batches += 1\n",
    "\n",
    "        # Log and print batch loss\n",
    "        wandb.log({\"train_batch_loss\": batch_loss / len(label_batch)})\n",
    "        progress_bar.set_postfix({'loss': running_loss / num_batches})\n",
    "\n",
    "    # wandb.log({\"train_loss\": running_loss / num_batches})\n",
    "    # print(f\"Epoch [{epoch+1}/{max_epochs}] - Train Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "\n",
    "def test(model, label_loader, max_num_negatives, temperature, norm):\n",
    "    \"\"\"Test the model and compute AUC, AUPRC, and MRR.\"\"\"\n",
    "    model.eval()\n",
    "    all_trues = []\n",
    "    all_predictions = []\n",
    "    all_ranks = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label_batch in label_loader:\n",
    "            batch_loss = 0.0\n",
    "            for variant, (gene, disease, traits) in label_batch.iterrows():\n",
    "                variant_id = variant_mapping[variant]\n",
    "                gene_id = gene_mapping[gene]\n",
    "\n",
    "                positive_relations = labels.loc[labels.index == variant, [\n",
    "                    'disease_index', 'hpo_id']]\n",
    "                positive_diseases = positive_relations['disease_index'].to_list(\n",
    "                )\n",
    "                positive_trait_groups = positive_relations['hpo_id'].to_list()\n",
    "\n",
    "                # Sample negative trait groups\n",
    "                negative_trait_groups = sample_negative_trait_groups(\n",
    "                    disease_to_traits, positive_diseases, max_num_negatives=max_num_negatives)\n",
    "                num_negatives = len(negative_trait_groups)\n",
    "\n",
    "                # Combine positive and negative trait groups\n",
    "                num_positives = len(positive_relations)\n",
    "                trait_groups = positive_trait_groups + negative_trait_groups\n",
    "                batch_ids = torch.cat([torch.full((len(group),), i)\n",
    "                                       for i, group in enumerate(trait_groups)])\n",
    "                trait_ids = torch.tensor(\n",
    "                    [trait_mapping[trait] for trait_group in trait_groups for trait in trait_group])\n",
    "\n",
    "                batch_ids = batch_ids.to(DEVICE)\n",
    "                trait_ids = trait_ids.to(DEVICE)\n",
    "\n",
    "                # Forward pass\n",
    "                gene_embedding, disease_embedding = model(\n",
    "                    variant_id=variant_id,\n",
    "                    gene_id=gene_id,\n",
    "                    trait_ids=trait_ids,\n",
    "                    batch_ids=batch_ids\n",
    "                )\n",
    "\n",
    "                loss = multi_positive_info_nce_loss(gene_embedding, disease_embedding, num_positives, temperature, norm)\n",
    "                batch_loss += loss.item()\n",
    "\n",
    "                # Cosine similarity (compute distance between gene embedding and disease embedding)\n",
    "                similarities = torch.cosine_similarity(\n",
    "                    gene_embedding, disease_embedding, dim=-1).cpu().numpy()\n",
    "\n",
    "                # Construct trues (positives first)\n",
    "                trues = [1] * num_positives + [0] * num_negatives\n",
    "\n",
    "                # Save the results\n",
    "                all_trues.extend(trues)\n",
    "                all_predictions.extend(similarities)\n",
    "\n",
    "                # Compute MRR (Mean Reciprocal Rank)\n",
    "                rank = 1 / (1 + torch.argsort(torch.tensor(similarities),\n",
    "                            descending=True).cpu().numpy().tolist().index(0))\n",
    "                all_ranks.append(rank)\n",
    "\n",
    "            wandb.log({\"valid_batch_loss\": batch_loss / len(label_batch)})\n",
    "    # Calculate AUROC, AUPRC, and MRR\n",
    "    auroc = roc_auc_score(all_trues, all_predictions)\n",
    "    auprc = average_precision_score(all_trues, all_predictions)\n",
    "    mrr = sum(all_ranks) / len(all_ranks)\n",
    "\n",
    "    wandb.log({\"AUROC\": auroc, \"AUPRC\": auprc, \"MRR\": mrr})\n",
    "    return auroc, auprc, mrr\n",
    "\n",
    "\n",
    "# Define the model\n",
    "set_seed_all(SEED)\n",
    "model = OurModel(\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_heads=num_heads,\n",
    "    num_graph_layers=num_graph_layers,\n",
    "    data=data,\n",
    "    variant_x=variant_x,\n",
    "    gene_local_x=gene_local_x,\n",
    "    pooling_type=pooling_type,\n",
    "    dropout_prob=dropout_prob\n",
    ").to(DEVICE)\n",
    "\n",
    "# Define optimizer hyperparameters\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr=learning_rate,\n",
    "#     weight_decay=weight_decay,\n",
    "#     betas=(momentum_gradient, momentum_square),\n",
    "# )\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    momentum=momentum_gradient,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "set_seed_all(SEED)\n",
    "train_loader = LabelGenerator(\n",
    "    train_labels, batch_size, num_samples=num_train_samples, shuffle=True)\n",
    "val_loader = LabelGenerator(\n",
    "    val_labels, batch_size, num_samples=num_valid_samples, shuffle=False)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # print(f\"\\nEpoch {epoch+1}/{max_epochs}\")\n",
    "    # wandb.log({\"epoch\": epoch + 1})\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        # print(f\"\\nValidation after Epoch {epoch+1}\")\n",
    "        test(model, val_loader, max_num_negatives, temperature, norm)\n",
    "\n",
    "    train(model, optimizer, train_loader,\n",
    "          max_num_negatives, max_positives, temperature, norm)\n",
    "\n",
    "# print(f\"\\nFinal Validation after Epoch {max_epochs}\")\n",
    "test(model, val_loader, max_num_negatives)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "wandb.save(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
